# -*- coding: utf-8 -*-
"""165_FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rFutXiYnBNINWnDRcNmfn7AEq-fFF59t

# Install and Import Requirements
"""

!pip install pandas numpy scikit-learn nltk spacy matplotlib vaderSentiment datasets
!python -m spacy download en_core_web_sm

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import spacy
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import nltk
nltk.download('stopwords')

"""# Load Data"""

from datasets import load_dataset

dataset = load_dataset("nikesh66/Sarcasm-dataset")
df = pd.DataFrame(dataset['train'])

df = df.rename(columns={"Tweet": "content", "Sarcasm (yes/no)": "label"})

df["label"] = df["label"].map({"yes": 1, "no": 0})

df.head()

# Step 2 — Clean Text
def clean_text(text):
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#\S+", "", text)
    text = re.sub(r"[^A-Za-z0-9\s]", "", text)
    text = re.sub(r"\s+", " ", text)
    return text.lower().strip()

df["clean_text"] = df["content"].apply(clean_text)

"""# Clean and Preprocess Data"""

# Step 3 — Sentiment features (VADER)
analyzer = SentimentIntensityAnalyzer()

df["sentiment"] = df["clean_text"].apply(lambda t: analyzer.polarity_scores(t)["compound"])

# Step 4 — POS features using spaCy
nlp = spacy.load("en_core_web_sm")

docs = list(nlp.pipe(df["clean_text"], batch_size=1000, disable=["ner"]))

def pos_ratio(doc, pos):
    return len([t for t in doc if t.pos_ == pos]) / (len(doc) + 1e-5)

df["adv_ratio"] = [pos_ratio(d, "ADV") for d in docs]
df["adj_ratio"] = [pos_ratio(d, "ADJ") for d in docs]

# Simple intensifiers list
intensifiers = ["literally", "totally", "seriously", "really"]
df["intensifiers"] = df["clean_text"].apply(lambda t: sum(word in t.split() for word in intensifiers))

# Step 5 — Punctuation / emphasis features
def punct_feats(text):
    return {
        "exclaim": text.count("!"),
        "caps": sum(1 for w in text.split() if w.isupper())
    }

df["exclaim"] = df["content"].apply(lambda x: punct_feats(str(x))["exclaim"])
df["caps"] = df["content"].apply(lambda x: punct_feats(str(x))["caps"])

"""# TF-IDF Vectorization"""

# Step 6 — TF-IDF vectorization
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_tfidf = tfidf.fit_transform(df["clean_text"])

# Step 7 — Combine numeric features with TF-IDF

numeric_features = df[["sentiment","adv_ratio","adj_ratio","intensifiers","exclaim","caps"]].values
X = hstack([X_tfidf, numeric_features])
y = df["label"].values

"""# Models"""

# Step 8 — Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Logistic Regression evaluation
lr = LogisticRegression(max_iter=500)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Logistic Regression:")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Precision:", precision_score(y_test, y_pred_lr))
print("Recall:", recall_score(y_test, y_pred_lr))
print("F1-score:", f1_score(y_test, y_pred_lr))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred_lr))
print("\n")

# SVM evaluation
svc = SVC()
svc.fit(X_train, y_train)
y_pred_svc = svc.predict(X_test)

print("SVM (linear kernel):")
print("Accuracy:", accuracy_score(y_test, y_pred_svc))
print("Precision:", precision_score(y_test, y_pred_svc))
print("Recall:", recall_score(y_test, y_pred_svc))
print("F1-score:", f1_score(y_test, y_pred_svc))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred_svc))
print("\n")

# --- Misclassified tweets for discussion ---
misclassified_idx = np.where(y_test != y_pred_lr)[0]
misclassified_tweets = df.iloc[misclassified_idx]

print("Example misclassified tweets (Logistic Regression):")
print(misclassified_tweets[["content", "label"]].head(5))

from sklearn.feature_extraction.text import CountVectorizer

X_train_text, X_test_text, y_train, y_test = train_test_split(
    df['clean_text'], df['label'], test_size=0.2, random_state=42
)

# Vectorize using CountVectorizer (no negative values ever)
vectorizer = CountVectorizer(stop_words='english')
X_train = vectorizer.fit_transform(X_train_text)
X_test = vectorizer.transform(X_test_text)

# Naive Bayes
nb = MultinomialNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)

print("Naive Bayes:")
print("Accuracy:", accuracy_score(y_test, y_pred_nb))
print("Precision:", precision_score(y_test, y_pred_nb))
print("Recall:", recall_score(y_test, y_pred_nb))
print("F1-score:", f1_score(y_test, y_pred_nb))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred_nb))

"""# Sentiment Analysis + Linguistic Features"""

# df["sentiment"], df["adv_ratio"], df["adj_ratio"], df["intensifiers"], df["exclaim"], df["label"]

df["label_text"] = df["label"].map({0: "Not Sarcastic", 1: "Sarcastic"})

# Sentiment Distribution Plot
plt.figure(figsize=(8,5))
sns.boxplot(data=df, x="label_text", y="sentiment")
plt.title("Sentiment Scores by Class")
plt.xlabel("")
plt.ylabel("Sentiment (VADER)")
plt.show()

# Linguistic Feature Comparison
feature_cols = ["adv_ratio", "adj_ratio", "intensifiers", "exclaim"]

df_melted = df[feature_cols + ["label_text"]].melt(id_vars="label_text")

plt.figure(figsize=(10,6))
sns.barplot(
    data=df_melted,
    x="variable",
    y="value",
    hue="label_text",
    errorbar=("ci", 95)
)
plt.title("Linguistic Feature Strength by Class")
plt.xlabel("Feature")
plt.ylabel("Average Value")
plt.show()

"""# Word Cloud - Sarcastic vs Non-Sarcastic"""

!pip install wordcloud

from wordcloud import WordCloud
import matplotlib.pyplot as plt

sarcastic_text = " ".join(df[df["label"] == 1]["clean_text"].tolist())
nonsarcastic_text = " ".join(df[df["label"] == 0]["clean_text"].tolist())

def show_wordcloud(text, title):
    wc = WordCloud(
        width=800,
        height=400,
        background_color="white",
        collocations=False
    ).generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.title(title, fontsize=18)
    plt.show()

# Sarcastic word cloud
show_wordcloud(sarcastic_text, "Word Cloud: Sarcastic Tweets")

# Non-sarcastic word cloud
show_wordcloud(nonsarcastic_text, "Word Cloud: Non-Sarcastic Tweets")

